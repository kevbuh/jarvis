{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolformer: Language Models Can Teach Themselves to Use Tools\n",
    "- https://arxiv.org/pdf/2302.04761.pdf\n",
    "\n",
    "Model trained to decide which APIs to call\n",
    "- when to call them\n",
    "- what arguments to pass\n",
    "- how to best incorporate the results into future token prediction\n",
    "\n",
    "Self-supervised\n",
    "- requiring nothing more than a handful of\n",
    "- demonstrations for each API\n",
    "\n",
    "Architecture\n",
    "- based on a pretrained GPT-J model with 6.7 Billion parameters\n",
    "- outperforming a GPT-3 model\n",
    "- https://github.com/kingoflolz/mesh-transformer-jax\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "Each api call as a tuple\n",
    "- ```c = (a_c, i_c)```\n",
    "- a_c -> name of the API\n",
    "- i_C -> corresponding input\n",
    "- corresponding result ```r```\n",
    "\n",
    "Special tokens\n",
    "- ```<API>``` and ```</API>```\n",
    "- ```-->```\n",
    "\n",
    "Dataset\n",
    "- Given a dataset of plain texts\n",
    "- Converted into a dataset augmented with API calls\n",
    "- Done in three steps\n",
    "    - 1. Exploit the in-context learning ability of the model ```M``` to sample a large numer of potential API calls\n",
    "    - 2. Execture these API calls and check whether the obtained repsonses are helpful for predicting future tokens \n",
    "        - Filtering criterion\n",
    "    - 3. Merge API calls for different tools, resulting in the augmented dataset ```C*```\n",
    "    - 4. Finetune ```M``` itself on this dataset\n",
    "    \n",
    "Sampling API Calls\n",
    "- write a prompt ```P(x)``` for each API\n",
    "    - encourage LM to annotate an example ```x=[x_1,...,x_n]``` with API calls\n",
    "  \n",
    "Executing API Calls\n",
    "- How the execution is done depends entirely on the API itself\n",
    "- Can involve\n",
    "    - Calling another neural network\n",
    "    - Executing a python script\n",
    "    - Using retrieval system to perform search or a large corpus\n",
    "- There is a response for each API call: ```c_i```\n",
    "    - needs to be a single text sequence ```r_i```\n",
    "    \n",
    "Filtering API Calls\n",
    "- Weighted cross entropy loss for ```M``` over the token x_i, ..., x_n if the model is prefixed with ```z```\n",
    "\n",
    "Model Finetuning\n",
    "- Construct new sequence ```x*=x_{1:i-1},e(c_i,r_c),x_{i:n}```\n",
    "- Doing this for all ```x``` in ```C``` results in the new dataset ```C*``` augmented with API calls\n",
    "- use this new dataset to finetune M\n",
    "\n",
    "Inference\n",
    "- Regular decoding until M produces ```-->```\n",
    "    - Indicates that is expects the response for an API call.\n",
    "- Interrupt the decoding process\n",
    "    - Call the appropriate API to get a response\n",
    "    - Continue the decoding process after inserting both the response and the ```</API>``` token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n",
    "\n",
    "Uses:\n",
    "- Question answering\n",
    "- calculator\n",
    "- wikipedia search\n",
    "- machine translation\n",
    "- calendar\n",
    "\n",
    "Contraints:\n",
    "- Both their inputs and outputs have to be represented as text sequences\n",
    "- We can obtain a few demonstrations of their intended use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "GPT-J\n",
    "- This paper uses it as the language model ```M```\n",
    "    - https://github.com/kingoflolz/mesh-transformer-jax\n",
    "- batch size of 128\n",
    "- learning rate of 1e-5\n",
    "- linear warmup for the first 10% of training\n",
    "    \n",
    "Datasets:\n",
    "- subset of CCNet as the language modeling dataset ```C```\n",
    "    - https://aclanthology.org/2020.lrec-1.494/\n",
    "    - To reduce the computational cost of\n",
    "annotating C with API calls, we define heuristics\n",
    "for some APIs to get a subset of C for which API\n",
    "calls are more likely to be helpful than for an average text. For example, we only consider texts\n",
    "for the calculator tool if they contain at least three\n",
    "numbers\n",
    "\n",
    "Benchmarking\n",
    "- SQuAD, Google-RE, and T-REx subsets of the LAMA benchmark\n",
    "    - https://aclanthology.org/D19-1250/\n",
    "    -  filter out examples where the mask token is not the final token, so that the remaining examples can be processed in a left-to-right fashion\n",
    "- Math\n",
    "    - ASDiv, SVAMP, MAWPS\n",
    "- Question Answering\n",
    "    - Web Questions, Natural Questions, TriviaQA\n",
    "    - mostly relying on the **Wikipedia search API (99.3%) to find relevant information**\n",
    "- Figure 4 shows that the ability to leverage the provided tools only emerges at around 775M parameters: smaller models achieve similar performance both with and without tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- 25k examples per api\n",
    "- max sequence length 1024\n",
    "- batch size of 128\n",
    "- trained using DeepSpeed's ZeRO-3 (Rasley et al., 2020)\n",
    "- 8 NVIDIA A100 40GB GPU's with BF16. \n",
    "- Training up to 2k steps\n",
    "- evaluate PPL on small dev set from CCNet containing 1k example every 500 steps.\n",
    "    - Pick the checkpoint that performs the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls.\n",
    "- this is done by finetuning on a large number of sampled API calss taht are filtered based on whether they reduce perplexity on future tokens.\n",
    "- considerebly imporoves zero shot performance of a 6.7B parameter GPT-J model\n",
    "    - outperforms GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
